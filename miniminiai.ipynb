{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import math, typing\n",
    "from collections.abc import Mapping\n",
    "from copy import copy\n",
    "from itertools import zip_longest\n",
    "from functools import partial, wraps\n",
    "from operator import attrgetter, itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import fastcore.all as fc\n",
    "from fastprogress import progress_bar, master_bar\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "from torcheval.metrics import Mean\n",
    "from einops import rearrange\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the most recent FastAI course ('Impractical Deep Learning for Coders') we built a library called 'miniai'. It's great - I've quickly adopted it as my go-to library for training models. BUT the core code is scattered across numerous notebooks, and it's not easy to find what you're looking for. Hence this notebook, which contains a complete, minimal version of miniai all in one place as a convenient reference. Let me know if you find it useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataloaders approach in miniai is much simpler than the fastai approach. Specifically, you are expected to pass in a pair of PyTorch dataloaders: one for training, and one for validation. \n",
    "\n",
    "Given a pair of datasets you can set up the dataloaders like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset needs to match the PyTorch dataset interface, which is very simple. It needs to have a `__len__` method, and a `__getitem__` method. The `__getitem__` method typically returns a tuple of (x, y) where x is the input and y is the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface datasets are convenient. They typically return dictionaries, so here's a collate function to handle them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def collate_dict(ds):\n",
    "    get = itemgetter(*ds.features)\n",
    "    def _f(b): return get(default_collate(b))\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain more about collate functions and the inplace thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def inplace(f):\n",
    "    def _f(b):\n",
    "        f(b)\n",
    "        return b\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DataLoaders:\n",
    "    def __init__(self, *dls): self.train,self.valid = dls[:2]\n",
    "\n",
    "    @classmethod\n",
    "    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n",
    "        f = collate_dict(dd['train'])\n",
    "        return cls(*get_dls(*dd.values(), bs=batch_size, collate_fn=f))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO fake dataloaders from style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for Displaying Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often work with images - this section has some convenient utilities for displaying them. At some point I'll add some examples :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.delegates(plt.Axes.imshow)\n",
    "def show_image(im, ax=None, figsize=None, title=None, noframe=True, **kwargs):\n",
    "    \"Show a PIL or PyTorch image on `ax`.\"\n",
    "    if fc.hasattrs(im, ('cpu','permute','detach')):\n",
    "        im = im.detach().cpu()\n",
    "        if len(im.shape)==3 and im.shape[0]<5: im=im.permute(1,2,0)\n",
    "    elif not isinstance(im,np.ndarray): im=np.array(im)\n",
    "    if im.shape[-1]==1: im=im[...,0]\n",
    "    if ax is None: _,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, **kwargs)\n",
    "    if title is not None: ax.set_title(title)\n",
    "    ax.set_xticks([]) \n",
    "    ax.set_yticks([]) \n",
    "    if noframe: ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "@fc.delegates(plt.subplots, keep=True)\n",
    "def subplots(\n",
    "    nrows:int=1, # Number of rows in returned axes grid\n",
    "    ncols:int=1, # Number of columns in returned axes grid\n",
    "    figsize:tuple=None, # Width, height in inches of the returned figure\n",
    "    imsize:int=3, # Size (in inches) of images that will be displayed in the returned figure\n",
    "    suptitle:str=None, # Title to be set to returned figure\n",
    "    **kwargs\n",
    "): # fig and axs\n",
    "    \"A figure and set of subplots to display images of `imsize` inches\"\n",
    "    if figsize is None: figsize=(ncols*imsize, nrows*imsize)\n",
    "    fig,ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n",
    "    if suptitle is not None: fig.suptitle(suptitle)\n",
    "    if nrows*ncols==1: ax = np.array([ax])\n",
    "    return fig,ax\n",
    "\n",
    "@fc.delegates(subplots)\n",
    "def get_grid(\n",
    "    n:int, # Number of axes\n",
    "    nrows:int=None, # Number of rows, defaulting to `int(math.sqrt(n))`\n",
    "    ncols:int=None, # Number of columns, defaulting to `ceil(n/rows)`\n",
    "    title:str=None, # If passed, title set to the figure\n",
    "    weight:str='bold', # Title font weight\n",
    "    size:int=14, # Title font size\n",
    "    **kwargs,\n",
    "): # fig and axs\n",
    "    \"Return a grid of `n` axes, `rows` by `cols`\"\n",
    "    if nrows: ncols = ncols or int(np.floor(n/nrows))\n",
    "    elif ncols: nrows = nrows or int(np.ceil(n/ncols))\n",
    "    else:\n",
    "        nrows = int(math.sqrt(n))\n",
    "        ncols = int(np.floor(n/nrows))\n",
    "    fig,axs = subplots(nrows, ncols, **kwargs)\n",
    "    for i in range(n, nrows*ncols): axs.flat[i].set_axis_off()\n",
    "    if title is not None: fig.suptitle(title, weight=weight, size=size)\n",
    "    return fig,axs\n",
    "\n",
    "@fc.delegates(subplots)\n",
    "def show_images(ims:list, # Images to show\n",
    "                nrows:typing.Union[int, None]=None, # Number of rows in grid\n",
    "                ncols:typing.Union[int, None]=None, # Number of columns in grid (auto-calculated if None)\n",
    "                titles:typing.Union[list, None]=None, # Optional list of titles for each image\n",
    "                **kwargs):\n",
    "    \"Show all images `ims` as subplots with `rows` using `titles`\"\n",
    "    axs = get_grid(len(ims), nrows, ncols, **kwargs)[1].flat\n",
    "    for im,t,ax in zip_longest(ims, titles or [], axs): show_image(im, ax=ax, title=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device\n",
    "\n",
    "Convenience functions related to device management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def to_device(x, device=def_device):\n",
    "    if isinstance(x, torch.Tensor): return x.to(device)\n",
    "    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n",
    "    return type(x)(to_device(o, device) for o in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    return x.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def collate_device(b): return to_device(default_collate(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Callback(): order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SingleBatchCB(Callback):\n",
    "    order = 1\n",
    "    def after_batch(self, learn): raise CancelFitException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MetricsCB(Callback):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        for o in ms: metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics['loss'] = self.loss = Mean()\n",
    "\n",
    "    def _log(self, d): print(d)\n",
    "    def before_fit(self, learn): learn.metrics = self\n",
    "    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n",
    "\n",
    "    def after_epoch(self, learn):\n",
    "        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n",
    "        log['epoch'] = learn.epoch\n",
    "        log['train'] = 'train' if learn.model.training else 'eval'\n",
    "        self._log(log)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): fc.store_attr()\n",
    "    def before_fit(self, learn):\n",
    "        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TrainCB(Callback):\n",
    "    def __init__(self, n_inp=1): self.n_inp = n_inp\n",
    "    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n",
    "    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class with_cbs:\n",
    "    def __init__(self, nm): self.nm = nm\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            try:\n",
    "                o.callback(f'before_{self.nm}')\n",
    "                f(o, *args, **kwargs)\n",
    "                o.callback(f'after_{self.nm}')\n",
    "            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n",
    "            finally: o.callback(f'cleanup_{self.nm}')\n",
    "        return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Learner():\n",
    "    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n",
    "        cbs = fc.L(cbs)\n",
    "        fc.store_attr()\n",
    "\n",
    "    @with_cbs('batch')\n",
    "    def _one_batch(self):\n",
    "        self.predict()\n",
    "        self.callback('after_predict')\n",
    "        self.get_loss()\n",
    "        self.callback('after_loss')\n",
    "        if self.training:\n",
    "            self.backward()\n",
    "            self.callback('after_backward')\n",
    "            self.step()\n",
    "            self.callback('after_step')\n",
    "            self.zero_grad()\n",
    "\n",
    "    @with_cbs('epoch')\n",
    "    def _one_epoch(self):\n",
    "        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n",
    "\n",
    "    def one_epoch(self, training):\n",
    "        self.model.train(training)\n",
    "        self.dl = self.dls.train if training else self.dls.valid\n",
    "        self._one_epoch()\n",
    "\n",
    "    @with_cbs('fit')\n",
    "    def _fit(self, train, valid):\n",
    "        for self.epoch in self.epochs:\n",
    "            if train: self.one_epoch(True)\n",
    "            if valid: torch.no_grad()(self.one_epoch)(False)\n",
    "\n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        # `add_cb` and `rm_cb` were added in lesson 18\n",
    "        for cb in cbs: self.cbs.append(cb)\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            if lr is None: lr = self.lr\n",
    "            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n",
    "            self._fit(train, valid)\n",
    "        finally:\n",
    "            for cb in cbs: self.cbs.remove(cb)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    @property\n",
    "    def training(self): return self.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TrainLearner(Learner):\n",
    "    def predict(self): self.preds = self.model(self.batch[0])\n",
    "    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "    def backward(self): self.loss.backward()\n",
    "    def step(self): self.opt.step()\n",
    "    def zero_grad(self): self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LRFinderCB(Callback):\n",
    "    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        self.sched = ExponentialLR(learn.opt, self.gamma)\n",
    "        self.lrs,self.losses = [],[]\n",
    "        self.min = math.inf\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if not learn.training: raise CancelEpochException()\n",
    "        self.lrs.append(learn.opt.param_groups[0]['lr'])\n",
    "        loss = to_cpu(learn.loss)\n",
    "        self.losses.append(loss)\n",
    "        if loss < self.min: self.min = loss\n",
    "        if loss > self.min*self.max_mult:\n",
    "            raise CancelFitException()\n",
    "        self.sched.step()\n",
    "\n",
    "    def cleanup_fit(self, learn):\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):\n",
    "    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BaseSchedCB(Callback):\n",
    "    def __init__(self, sched): self.sched = sched\n",
    "    def before_fit(self, learn): self.schedo = self.sched(learn.opt)\n",
    "    def _step(self, learn):\n",
    "        if learn.training: self.schedo.step()\n",
    "        \n",
    "class BatchSchedCB(BaseSchedCB):\n",
    "    def after_batch(self, learn): self._step(learn)\n",
    "\n",
    "class HasLearnCB(Callback):\n",
    "    def before_fit(self, learn): self.learn = learn \n",
    "    def after_fit(self, learn): self.learn = None\n",
    "\n",
    "class RecorderCB(Callback):\n",
    "    def __init__(self, **d): self.d = d\n",
    "    def before_fit(self, learn):\n",
    "        self.recs = {k:[] for k in self.d}\n",
    "        self.pg = learn.opt.param_groups[0]\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        if not learn.training: return\n",
    "        for k,v in self.d.items():\n",
    "            self.recs[k].append(v(self))\n",
    "\n",
    "    def plot(self):\n",
    "        for k,v in self.recs.items():\n",
    "            plt.plot(v, label=k)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "class EpochSchedCB(BaseSchedCB):\n",
    "    def after_epoch(self, learn): self._step(learn)\n",
    "\n",
    "# TODO tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "\n",
    "class MixedPrecision(TrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    \n",
    "    def before_fit(self, learn): self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        self.autocast = torch.autocast(\"cuda\", dtype=torch.float16)\n",
    "        self.autocast.__enter__()\n",
    "\n",
    "    def after_loss(self, learn): self.autocast.__exit__(None, None, None)\n",
    "        \n",
    "    def backward(self, learn): self.scaler.scale(learn.loss).backward()\n",
    "\n",
    "    def step(self, learn):\n",
    "        self.scaler.step(learn.opt)\n",
    "        self.scaler.update()\n",
    "\n",
    "class AccelerateCB(TrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    def __init__(self, n_inp=1, mixed_precision=\"fp16\"):\n",
    "        super().__init__(n_inp=n_inp)\n",
    "        self.acc = Accelerator(mixed_precision=mixed_precision)\n",
    "        \n",
    "    def before_fit(self, learn):\n",
    "        learn.model,learn.opt,learn.dls.train,learn.dls.valid = self.acc.prepare(\n",
    "            learn.model, learn.opt, learn.dls.train, learn.dls.valid)\n",
    "\n",
    "    def backward(self, learn): self.acc.backward(learn.loss)\n",
    "\n",
    "    def after_fit(self, learn):\n",
    "        learn.model = self.acc.unwrap_model(learn.model)\n",
    "\n",
    "# TODO tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model components\n",
    "\n",
    "Not sure I'll keep these in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def conv(ni, nf, ks=3, stride=2, act=True):\n",
    "    res = nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2)\n",
    "    if act: res = nn.Sequential(res, nn.ReLU())\n",
    "    return res\n",
    "\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x -= self.sub\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "\n",
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "\n",
    "def _conv_block(ni, nf, stride, act=act_gr, norm=None, ks=3):\n",
    "    return nn.Sequential(conv(ni, nf, stride=1, act=act, norm=norm, ks=ks),\n",
    "                         conv(nf, nf, stride=stride, act=None, norm=norm, ks=ks))\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride=1, ks=3, act=act_gr, norm=None):\n",
    "        super().__init__()\n",
    "        self.convs = _conv_block(ni, nf, stride, act=act, ks=ks, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else conv(ni, nf, ks=1, stride=1, act=None)\n",
    "        self.pool = fc.noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x): return self.act(self.convs(x) + self.idconv(self.pool(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion-related bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def abar(t): return (t*math.pi/2).cos()**2\n",
    "def inv_abar(x): return x.sqrt().acos()*2/math.pi\n",
    "\n",
    "def noisify(x0):\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.rand(n,).to(x0).clamp(0,0.999)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    abar_t = abar(t).reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = abar_t.sqrt()*x0 + (1-abar_t).sqrt()*ε\n",
    "    return (xt, t.to(device)), ε\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[xl])\n",
    "def dl_ddpm(ds, bs=32): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)\n",
    "\n",
    "def timestep_embedding(tsteps, emb_dim, max_period= 10000):\n",
    "    exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)\n",
    "    emb = tsteps[:,None].float() * exponent.exp()[None,:]\n",
    "    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "    return F.pad(emb, (0,1,0,0)) if emb_dim%2==1 else emb\n",
    "\n",
    "def pre_conv(ni, nf, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return layers\n",
    "\n",
    "def upsample(nf): return nn.Sequential(nn.Upsample(scale_factor=2.), nn.Conv2d(nf, nf, 3, padding=1))\n",
    "\n",
    "def lin(ni, nf, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Linear(ni, nf, bias=bias))\n",
    "    return layers\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans, transpose=True):\n",
    "        super().__init__()\n",
    "        self.nheads = ni//attn_chans\n",
    "        self.scale = math.sqrt(ni/self.nheads)\n",
    "        self.norm = nn.LayerNorm(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "        self.t = transpose\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n,c,s = x.shape\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x)\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class SelfAttention2D(SelfAttention):\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        return super().forward(x.view(n, c, -1)).reshape(n,c,h,w)\n",
    "\n",
    "class EmbResBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d, attn_chans=0):\n",
    "        super().__init__()\n",
    "        if nf is None: nf = ni\n",
    "        self.emb_proj = nn.Linear(n_emb, nf*2)\n",
    "        self.conv1 = pre_conv(ni, nf, ks, act=act, norm=norm)\n",
    "        self.conv2 = pre_conv(nf, nf, ks, act=act, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else nn.Conv2d(ni, nf, 1)\n",
    "        self.attn = False\n",
    "        if attn_chans: self.attn = SelfAttention2D(nf, attn_chans)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inp = x\n",
    "        x = self.conv1(x)\n",
    "        emb = self.emb_proj(F.silu(t))[:, :, None, None]\n",
    "        scale,shift = torch.chunk(emb, 2, dim=1)\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv2(x)\n",
    "        x = x + self.idconv(inp)\n",
    "        if self.attn: x = x + self.attn(x)\n",
    "        return x\n",
    "\n",
    "def saved(m, blk):\n",
    "    m_ = m.forward\n",
    "\n",
    "    @wraps(m.forward)\n",
    "    def _f(*args, **kwargs):\n",
    "        res = m_(*args, **kwargs)\n",
    "        blk.saved.append(res)\n",
    "        return res\n",
    "\n",
    "    m.forward = _f\n",
    "    return m\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf, add_down=True, num_layers=1, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList([saved(EmbResBlock(n_emb, ni if i==0 else nf, nf, attn_chans=attn_chans), self)\n",
    "                                      for i in range(num_layers)])\n",
    "        self.down = saved(nn.Conv2d(nf, nf, 3, stride=2, padding=1), self) if add_down else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.saved = []\n",
    "        for resnet in self.resnets: x = resnet(x, t)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, prev_nf, nf, add_up=True, num_layers=2, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList(\n",
    "            [EmbResBlock(n_emb, (prev_nf if i==0 else nf)+(ni if (i==num_layers-1) else nf), nf, attn_chans=attn_chans)\n",
    "            for i in range(num_layers)])\n",
    "        self.up = upsample(nf) if add_up else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t, ups):\n",
    "        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1), t)\n",
    "        return self.up(x)\n",
    "\n",
    "class EmbUNetModel(nn.Module):\n",
    "    def __init__( self, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1, attn_chans=8, attn_start=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf*4\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        n = len(nfs)\n",
    "        for i in range(n):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=n-1, num_layers=num_layers,\n",
    "                                        attn_chans=0 if i<attn_start else attn_chans))\n",
    "        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n",
    "\n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(n):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=n-1, num_layers=num_layers+1,\n",
    "                                    attn_chans=0 if i>=n-attn_start else attn_chans))\n",
    "        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x,t = inp\n",
    "        temb = timestep_embedding(t, self.n_temb)\n",
    "        emb = self.emb_mlp(temb)\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb)\n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)\n",
    "\n",
    "def ddim_step(x_t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta, sig, clamp=True):\n",
    "    sig = ((bbar_t1/bbar_t).sqrt() * (1-abar_t/abar_t1).sqrt()) * eta\n",
    "    x_0_hat = ((x_t-(1-abar_t).sqrt()*noise) / abar_t.sqrt())\n",
    "    if clamp: x_0_hat = x_0_hat.clamp(-1,1)\n",
    "    if bbar_t1<=sig**2+0.01: sig=0.  # set to zero if very small or NaN\n",
    "    x_t = abar_t1.sqrt()*x_0_hat + (bbar_t1-sig**2).sqrt()*noise\n",
    "    x_t += sig * torch.randn(x_t.shape).to(x_t)\n",
    "    return x_0_hat,x_t\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(f, model, sz, steps, eta=1., clamp=True):\n",
    "    model.eval()\n",
    "    ts = torch.linspace(1-1/steps,0,steps)\n",
    "    x_t = torch.randn(sz).cuda()\n",
    "    preds = []\n",
    "    for i,t in enumerate(progress_bar(ts)):\n",
    "        t = t[None].cuda()\n",
    "        abar_t = abar(t)\n",
    "        noise = model((x_t, t))\n",
    "        abar_t1 = abar(t-1/steps) if t>=1/steps else torch.tensor(1)\n",
    "        x_0_hat,x_t = f(x_t, noise, abar_t, abar_t1, 1-abar_t, 1-abar_t1, eta, 1-((i+1)/100), clamp=clamp)\n",
    "        preds.append(x_0_hat.float().cpu())\n",
    "    return preds\n",
    "\n",
    "@torch.no_grad()\n",
    "def cond_sample(c, f, model, sz, steps, eta=1.):\n",
    "    ts = torch.linspace(1-1/steps,0,steps)\n",
    "    x_t = torch.randn(sz).cuda()\n",
    "    c = x_t.new_full((sz[0],), c, dtype=torch.int32)\n",
    "    preds = []\n",
    "    for i,t in enumerate(progress_bar(ts)):\n",
    "        t = t[None].cuda()\n",
    "        abar_t = abar(t)\n",
    "        noise = model((x_t, t, c))\n",
    "        abar_t1 = abar(t-1/steps) if t>=1/steps else torch.tensor(1)\n",
    "        x_0_hat,x_t = f(x_t, noise, abar_t, abar_t1, 1-abar_t, 1-abar_t1, eta, 1-((i+1)/100))\n",
    "        preds.append(x_0_hat.float().cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO resblocks etc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting for Use Elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export to a file:\n",
    "# nb_export('miniminiai.ipynb', lib_path='.', name='miniminiai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a proper module\n",
    "!mkdir -p miniminiai\n",
    "!touch miniminiai/__init__.py\n",
    "nb_export('miniminiai.ipynb', lib_path='miniminiai', name='miniminiai')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
